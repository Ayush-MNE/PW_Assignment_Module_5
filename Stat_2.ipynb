{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question1: Define the z-statistic and explain its relationship to the standard normal distribution. How is the\n",
    "z-statistic used in hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The z-statistic, or z-score, measures how many standard deviations a data point is from the mean of a distribution. It is calculated using the formula:\n",
    "\n",
    "ùëß = (ùëã ‚àí ùúá)/ ùúé\n",
    "where:\n",
    "\n",
    "X is the value being standardized,\n",
    "Œº is the population mean,\n",
    "œÉ is the population standard deviation.\n",
    "In the context of a sampling distribution of a sample mean, the z-statistic is given by:\n",
    "ùëß = (ùëã ‚àí ùúá)/ (ùúé/n 0.5)\n",
    "\n",
    "n is the sample size.\n",
    "\n",
    "Relationship to the Standard Normal Distribution\n",
    "The z-statistic follows the standard normal distribution, which is a normal distribution with a mean of 0 and a standard deviation of 1. This distribution is symmetric about the mean and is used as a reference for standardizing scores. When a z-score is computed, it indicates how far away a value is from the mean in terms of standard deviations, providing a standardized way to compare different data points.\n",
    "\n",
    "Z-Statistic in Hypothesis Testing\n",
    "In hypothesis testing, the z-statistic is used to test whether a sample mean is significantly different from a known population mean. The steps are as follows:\n",
    "\n",
    "State the null hypothesis (ùêª0): Usually, this hypothesis states that there is no effect or no difference, implying that the sample mean is equal to the population mean.\n",
    "\n",
    "Calculate the z-statistic: Determine the z-score using the sample data.\n",
    "\n",
    "Determine the critical value(s): Based on the significance level (commonly ùõº = 0.05), find the critical z-value(s) from the standard normal distribution that mark the rejection region(s).\n",
    "\n",
    "Compare the calculated z-statistic to the critical value(s): If the z-statistic falls in the rejection region (beyond the critical values), the null hypothesis is rejected.\n",
    "\n",
    "Conclusion: Rejecting the null hypothesis suggests that the observed difference is statistically significant. If the z-statistic does not fall within the rejection region, the null hypothesis is not rejected.\n",
    "\n",
    "The z-statistic provides a basis for understanding the probability of observing a given result under the assumption that the null hypothesis is true. This probability is known as the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question2 : What is a p-value, and how is it used in hypothesis testing? What does it mean if the p-value is\n",
    "very small (e.g., 0.01)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. A p-value is the probability of obtaining test results at least as extreme as the observed results, assuming that the null hypothesis is true. It quantifies the evidence against the null hypothesis in a statistical test.\n",
    "\n",
    "Use of p-Value in Hypothesis Testing\n",
    "In hypothesis testing, the p-value is used to decide whether to reject the null hypothesis (ùêª0). The steps include:\n",
    "\n",
    "State the null hypothesis (ùêª0) and the alternative hypothesis (ùêª1): The null hypothesis generally represents no effect or no difference, while the alternative hypothesis represents the opposite.\n",
    "\n",
    "Calculate the test statistic (e.g., z-statistic or t-statistic): This measures how far the observed data deviate from the null hypothesis.\n",
    "\n",
    "Determine the p-value: The p-value is calculated based on the test statistic and represents the probability of observing a result at least as extreme as the test statistic if the null hypothesis is true.\n",
    "\n",
    "Compare the p-value to the significance level (Œ±): The significance level is a pre-determined threshold, usually set at 0.05 (5%).\n",
    "\n",
    "If p-value ‚â§ \n",
    "Œ±: Reject the null hypothesis. This indicates that the observed data are unlikely to occur under the null hypothesis, providing evidence in favor of the alternative hypothesis.\n",
    "If p-value > \n",
    "ùõº\n",
    "Œ±: Do not reject the null hypothesis. This suggests that there isn't sufficient evidence to conclude that the null hypothesis is false.\n",
    "Interpretation of a Very Small p-Value (e.g., 0.01)\n",
    "A small p-value (e.g., 0.01) indicates strong evidence against the null hypothesis. In this case:\n",
    "\n",
    "p-value = 0.01 implies that there is only a 1% probability of obtaining the observed results (or more extreme results) if the null hypothesis is true.\n",
    "When the p-value is very small, it suggests that the observed data are highly inconsistent with the null hypothesis, providing strong evidence to reject it.\n",
    "Thus, if the significance level is set at 0.05 and the p-value is 0.01, the null hypothesis would be rejected, suggesting that the results are statistically significant and that there is likely a real effect or difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question3: Compare and contrast the binomial and Bernoulli distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The binomial and Bernoulli distributions are both discrete probability distributions that describe outcomes of experiments with two possible results, often referred to as \"success\" and \"failure.\" However, they differ in their scope and application.\n",
    "\n",
    "Bernoulli Distribution\n",
    "Definition: The Bernoulli distribution represents a single trial of an experiment that has only two possible outcomes: success (usually coded as 1) with probability p, and failure (coded as 0) with probability 1‚àíp.\n",
    "\n",
    "Parameters: The Bernoulli distribution is characterized by a single parameter p, which represents the probability of success.\n",
    "\n",
    "Mean: Œº=p\n",
    "\n",
    "Variance:  ùúé=p(1‚àíp)\n",
    "\n",
    "Use Case: It is used for modeling the outcome of a single binary (yes/no) event, such as flipping a coin or passing/failing a test.\n",
    "\n",
    "Binomial Distribution\n",
    "Definition: The binomial distribution represents the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success p.\n",
    "Parameters: It is characterized by two parameters: ùëõ\n",
    "n: the number of trials\n",
    "p: the probability of success in each trial\n",
    "Mean: Œº=np\n",
    "\n",
    "Variance: ùúé =np(1‚àíp)\n",
    "\n",
    "Use Case: It is used for modeling the number of successes in a fixed number of independent trials, such as the number of heads in 10 coin flips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Under what conditions is the binomial distribution used, and how does it relate to the Bernoulli\n",
    "distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The binomial distribution is used under specific conditions that characterize a series of binary outcome experiments. These conditions ensure that the distribution accurately models the probability of a certain number of successes in a sequence of trials.\n",
    "\n",
    "Conditions for Using the Binomial Distribution\n",
    "The binomial distribution applies when all the following conditions are met:\n",
    "\n",
    "Fixed Number of Trials (n): There is a set number of trials, denoted by n, where each trial is an independent repetition of the same experiment.\n",
    "\n",
    "Two Possible Outcomes (Success or Failure): Each trial has only two possible outcomes, typically labeled as \"success\" and \"failure.\"\n",
    "\n",
    "Constant Probability of Success (p): The probability of success remains the same for each trial. This probability is denoted by p, while the probability of failure is 1‚àíp.\n",
    "\n",
    "Independence of Trials: The outcome of each trial is independent of the outcomes of other trials. This means that the result of one trial does not affect the probability of success or failure in another trial.\n",
    "\n",
    "Relationship to the Bernoulli Distribution\n",
    "The binomial distribution is closely related to the Bernoulli distribution in the following ways:\n",
    "\n",
    "Sum of Independent Bernoulli Trials: The binomial distribution represents the number of successes in a fixed number of independent Bernoulli trials. Each trial is a Bernoulli random variable with the same probability of success p. Therefore, if we conduct n Bernoulli trials, the total number of successes follows a binomial distribution.\n",
    "\n",
    "Special Case: When n=1, the binomial distribution reduces to the Bernoulli distribution. In this case, the binomial distribution describes a single trial, which has only two possible outcomes (success or failure), matching the Bernoulli distribution.\n",
    "\n",
    "Example to Illustrate the Relationship\n",
    "Consider flipping a fair coin:\n",
    "\n",
    "A Bernoulli distribution would model a single coin flip, where \"success\" might be getting a head (p=0.5) and \"failure\" is getting a tail (1‚àíp=0.5).\n",
    "A binomial distribution would model the number of heads obtained if the coin is flipped \n",
    "ùëõ\n",
    "n times. Here, n is the number of trials, and p=0.5 remains the probability of getting a head in each trial.\n",
    "In summary, the binomial distribution extends the Bernoulli distribution to multiple trials, making it suitable for modeling the probability of a specific number of successes across several independent experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question5: What are the key properties of the Poisson distribution, and when is it appropriate to use this\n",
    "distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. he Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space, provided that these events happen with a known average rate and independently of the time since the last event.\n",
    "\n",
    "Key Properties of the Poisson Distribution\n",
    "Probability Mass Function (PMF): The probability of observing exactly k events in a fixed interval is given by:\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "Œª is the average rate (mean) of events occurring in the interval,\n",
    "\n",
    "k is the number of events (a non-negative integer),\n",
    "\n",
    "e is the base of the natural logarithm (approximately 2.71828).\n",
    "Mean and Variance:\n",
    "\n",
    "The mean of the Poisson distribution is Œª.\n",
    "The variance is also Œª.\n",
    "This property implies that the mean and variance are equal, which is a unique characteristic of the Poisson distribution.\n",
    "Additivity: If ùëã1 and ùëã2 are two independent Poisson random variables with rates ùúÜ1  and ùúÜ2 , then their sum ùëã1 +ùëã2  is also Poisson-distributed with rate ùúÜ1 + ùúÜ2.\n",
    "\n",
    "Memorylessness: The Poisson distribution is often used to model processes with the memoryless property, where the occurrence of events in one interval does not affect the occurrence in another interval.\n",
    "\n",
    "When to Use the Poisson Distribution\n",
    "The Poisson distribution is appropriate to use under the following conditions:\n",
    "\n",
    "Counting the Number of Events in a Fixed Interval: The distribution models the number of events (e.g., arrivals, accidents, calls) occurring within a fixed period of time, distance, area, or volume.\n",
    "\n",
    "Events Occur Independently: The occurrence of one event does not affect the probability of another event occurring. Each event happens independently of others.\n",
    "\n",
    "Constant Average Rate (Œª): The average rate of occurrence is known and remains constant throughout the fixed interval. This rate is represented by the parameter Œª, which denotes the expected number of events in the interval.\n",
    "\n",
    "Rare Events: The Poisson distribution is often used for modeling rare events, where the probability of more than one event occurring in a very short interval is negligible.\n",
    "\n",
    "Examples of Appropriate Use Cases\n",
    "Counting the number of phone calls received by a call center in an hour.\n",
    "Modeling the number of accidents occurring at a specific intersection in a month.\n",
    "Estimating the number of defects in a length of fabric or a batch of products.\n",
    "Predicting the occurrence of earthquakes in a region over a given time period.\n",
    "The Poisson distribution is particularly useful for modeling scenarios where events occur sporadically or unpredictably but follow a known average rate.\n",
    " \n",
    "‚Äã\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question6: Define the terms \"probability distribution\" and \"probability density function\" (PDF). How does a\n",
    "PDF differ from a probability mass function (PMF)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Probability Distribution\n",
    "A probability distribution describes how the probabilities of different possible outcomes of a random variable are distributed. It provides a mathematical function that gives the probability of each outcome in the sample space, for both discrete and continuous random variables.\n",
    "\n",
    "For a discrete random variable, the probability distribution lists the probabilities of all possible values the variable can take.\n",
    "For a continuous random variable, the probability distribution describes the probabilities over an interval of values using a function.\n",
    "Probability Density Function (PDF)\n",
    "A probability density function (PDF) is a function used to specify the probability distribution of a continuous random variable. The PDF, denoted as f(x), describes the relative likelihood of the random variable taking on a particular value.\n",
    "\n",
    "Key properties of a PDF:\n",
    "\n",
    "Non-negativity: f(x)‚â•0 for all x.\n",
    "Normalization: The total area under the curve of the PDF over the entire range of the random variable is equal to 1.\n",
    "Probability of an interval: The probability that the random variable falls within a specific interval [a,b] is given by the integral of the PDF over that interval:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question7: Explain the Central Limit Theorem (CLT) with example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The Central Limit Theorem (CLT) is a fundamental statistical theorem that states that, for a sufficiently large sample size, the distribution of the sample mean of independent and identically distributed (i.i.d.) random variables approaches a normal distribution, regardless of the original distribution of the population.\n",
    "\n",
    "Key Points of the Central Limit Theorem:\n",
    "Independent and Identically Distributed (i.i.d.) Variables: The random variables must be independent, meaning the outcome of one does not affect the others, and identically distributed, meaning they are drawn from the same probability distribution.\n",
    "\n",
    "Large Sample Size (n): As the sample size increases, the distribution of the sample mean becomes closer to a normal distribution. In practice, a sample size of n‚â•30 is often considered sufficient for the CLT to hold.\n",
    "\n",
    "Original Distribution's Shape is Irrelevant: The original population distribution can be normal, skewed, uniform, or any other shape. The CLT still applies as the sample size becomes large.\n",
    "\n",
    "Mean and Standard Deviation of the Sampling Distribution:\n",
    "\n",
    "The mean of the sampling distribution of the sample mean is equal to the population mean (Œº).\n",
    "The standard deviation of the sampling distribution (known as the standard error) is equal to the population standard deviation (œÉ) divided by the square root of the sample size (ùëõ):\n",
    "Standard¬†Error=ùúé/ùëõ0.5\n",
    "Example Illustrating the Central Limit Theorem\n",
    "Suppose we are interested in the average number of hours people sleep per night in a city. The true distribution of sleep hours may be skewed (e.g., more people may sleep fewer hours than the average).\n",
    "\n",
    "Let's say the population has a mean sleep time of Œº=7 hours, and the standard deviation is œÉ=2 hours.\n",
    "\n",
    "Step-by-Step Illustration:\n",
    "Draw Samples: We take multiple random samples of size n=30, 50, or 100 from this population.\n",
    "\n",
    "Calculate Sample Means: For each sample, calculate the mean sleep time.\n",
    "\n",
    "Distribution of Sample Means: As we increase the number of samples, the distribution of these sample means starts to resemble a normal distribution, even though the original distribution of sleep hours may be skewed.\n",
    "\n",
    "Properties of the Sampling Distribution:\n",
    "\n",
    "The mean of the distribution of sample means will be approximately 7 hours.\n",
    "The standard error will be 2/30**0.5 ‚âà 0.365 hours for n=30.\n",
    "Outcome: With a sufficiently large number of samples, the distribution of the sample means will look like a bell-shaped curve (normal distribution), even if the original sleep distribution is not normal.\n",
    "\n",
    "Importance of the Central Limit Theorem\n",
    "Basis for Inferential Statistics: The CLT enables us to make inferences about the population mean using sample data, as many statistical methods assume normality in the sampling distribution.\n",
    "Confidence Intervals and Hypothesis Testing: The theorem is used to construct confidence intervals for the population mean and perform hypothesis tests, even when the original population distribution is unknown.\n",
    "In summary, the Central Limit Theorem states that the distribution of the sample mean approximates a normal distribution as the sample size becomes large, making it a powerful tool for statistical inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question8: Compare z-scores and t-scores. When should you use a z-score, and when should a t-score be applied instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Z-scores and t-scores are both measures used in statistics to assess how far a data point is from the mean in terms of standard deviations. However, they differ in their applications, particularly when it comes to sample size and knowledge of the population parameters.\n",
    "\n",
    "Z-Score\n",
    "A z-score measures how many standard deviations a data point is from the mean of a population. It is calculated using the formula:\n",
    "z= (X‚àíŒº)/œÉ\n",
    "\n",
    "‚Äã\n",
    " \n",
    "where:\n",
    "\n",
    "\n",
    "X is the data point,\n",
    "Œº is the population mean,\n",
    "œÉ is the population standard deviation.\n",
    "The z-score assumes that the data follows a normal distribution and that the population standard deviation is known. It is used when working with large sample sizes (typically n‚â•30) or when the population standard deviation is available. The z-score is appropriate for cases where the distribution of the sample mean is approximately normal.\n",
    "\n",
    "T-Score\n",
    "A t-score is used when the sample size is small (n<30) or when the population standard deviation is unknown. The formula for calculating the t-score is:\n",
    "\n",
    "ùë° = (X‚àíŒº)/ (S/n**0.5) \n",
    "where:\n",
    "\n",
    "X is the data point,\n",
    "XÀâ  is the sample mean,\n",
    "\n",
    "s is the sample standard deviation,\n",
    "\n",
    "n is the sample size.\n",
    "The t-score accounts for the additional variability that arises from estimating the population standard deviation using the sample. As such, the t-distribution is wider and has heavier tails than the standard normal distribution, especially for small sample sizes. As the sample size increases, the t-distribution approaches the normal distribution.\n",
    "\n",
    "When to Use a Z-Score\n",
    "Use a z-score when:\n",
    "\n",
    "The sample size is large (n‚â•30), making the sample distribution approximately normal.\n",
    "The population standard deviation is known, allowing for a more precise measure of the spread of data.\n",
    "When to Use a T-Score\n",
    "Use a t-score when:\n",
    "\n",
    "The sample size is small (n<30), leading to greater variability in the estimation.\n",
    "The population standard deviation is unknown, requiring the use of the sample standard deviation as an estimate.\n",
    "In summary, choose a z-score for large samples or known population standard deviation, and use a t-score for small samples or when the population standard deviation is unknown. The t-distribution adjusts for the extra uncertainty in small samples, while the z-distribution is used under more stable, well-known conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question9: Given a sample mean of 105, a population mean of 100, a standard deviation of 15, and a sample\n",
    "size of 25, calculate the z-score and p-value. Based on a significance level of 0.05, do you reject or fail to\n",
    "reject the null hypothesis?\n",
    "\n",
    " Task: Write Python code to calculate the z-score and p-value for the given data.\n",
    "\n",
    "Objective: Apply the formula for the z-score and interpret the p-value for hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "# Given data\n",
    "sample_mean = 105\n",
    "population_mean = 100\n",
    "standard_deviation = 15\n",
    "sample_size = 25\n",
    "\n",
    "# Step 1: Calculate the z-score\n",
    "z_score = (sample_mean - population_mean) / (standard_deviation / math.sqrt(sample_size))\n",
    "\n",
    "# Step 2: Calculate the p-value\n",
    "# Since this is a two-tailed test, we multiply the tail probability by 2\n",
    "p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
    "\n",
    "# Step 3: Determine whether to reject the null hypothesis\n",
    "significance_level = 0.05\n",
    "reject_null = p_value < significance_level\n",
    "\n",
    "# Output the results\n",
    "print(f\"Z-Score: {z_score:.2f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "print(f\"Reject the null hypothesis: {reject_null}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question10: Simulate a binomial distribution with 10 trials and a probability of success of 0.6 using Python.\n",
    "Generate 1,000 samples and plot the distribution. What is the expected mean and variance?\n",
    "\n",
    "Task: Use Python to generate the data, plot the distribution, and calculate the mean and variance.\n",
    "\n",
    "Objective: Understand the properties of a binomial distribution and verify them through simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Given parameters\n",
    "n = 10       # number of trials\n",
    "p = 0.6      # probability of success\n",
    "num_samples = 1000  # number of samples to generate\n",
    "\n",
    "# Step 1: Simulate the binomial distribution\n",
    "samples = np.random.binomial(n, p, num_samples)\n",
    "\n",
    "# Step 2: Plot the distribution\n",
    "plt.hist(samples, bins=range(n + 2), edgecolor='black', alpha=0.7)\n",
    "plt.title('Binomial Distribution (n=10, p=0.6)')\n",
    "plt.xlabel('Number of Successes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Calculate the mean and variance of the simulated data\n",
    "simulated_mean = np.mean(samples)\n",
    "simulated_variance = np.var(samples)\n",
    "\n",
    "# Step 4: Theoretical mean and variance\n",
    "theoretical_mean = n * p\n",
    "theoretical_variance = n * p * (1 - p)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Simulated Mean: {simulated_mean:.2f}\")\n",
    "print(f\"Simulated Variance: {simulated_variance:.2f}\")\n",
    "print(f\"Theoretical Mean: {theoretical_mean}\")\n",
    "print(f\"Theoretical Variance: {theoretical_variance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
